@attr(tags=['advanced', 'basic'], required_hardware='True')
def test_01_migrate_live(self):
    ' Test migrate Volume (root and data disk)\n\n        # 1. Deploy a VM on cluster wide primary storage.\n        # 2. Migrate root and data volume to two different storage pools             in same cluster.\n\n        '
    try:
        self.pools = StoragePool.list(self.apiclient, zoneid=self.zone.id)
        assert (len(list((storagePool for storagePool in self.pools if (storagePool.scope == 'CLUSTER')))) >= 3), 'There must be at least three cluster wide                storage pools available in the setup'
    except Exception as e:
        self.skipTest(e)
    storagePools_to_avoid = []
    cluster_no = 1
    for storagePool in self.pools:
        if (storagePool.scope == 'CLUSTER'):
            StoragePool.update(self.apiclient, id=storagePool.id, tags=[('cwps' + repr(cluster_no))])
        cluster_no += 1
    vm_cluster = VirtualMachine.create(self.userapiclient, self.testdata['small'], templateid=self.template.id, accountid=self.account.name, domainid=self.account.domainid, serviceofferingid=self.service_offering_cluster1.id, zoneid=self.zone.id, mode=self.zone.networktype)
    root_volumes_cluster_list = list_volumes(self.apiclient, virtualmachineid=vm_cluster.id, type='ROOT', listall=True)
    root_volume_cluster = root_volumes_cluster_list[0]
    data_volume_clust_1 = Volume.create(self.apiclient, self.testdata['volume'], zoneid=self.zone.id, account=self.account.name, domainid=self.account.domainid, diskofferingid=self.disk_offering_cluster1.id)
    self.debug(('Created volume with ID: %s' % data_volume_clust_1.id))
    vm_cluster.attach_volume(self.userapiclient, data_volume_clust_1)
    data_disk_1_volumes_cluster_list = Volume.list(self.userapiclient, listall=self.testdata['listall'], id=data_volume_clust_1.id)
    data_volume_1_cluster = data_disk_1_volumes_cluster_list[0]
    checksum_random_root_cluster = createChecksum(self, vm_cluster, root_volume_cluster, 'rootdiskdevice')
    storagePools_to_avoid = [root_volume_cluster.storage]
    destinationPool_for_root_disk = GetDestinationPool(self, storagePools_to_avoid, 'CLUSTER')
    MigrateDataVolume(self, root_volume_cluster, destinationPool_for_root_disk, islive=True)
    compareChecksum(self, checksum_random_root_cluster, 'rootdiskdevice', virt_machine=vm_cluster, disk=None)
    checksum_random_data_cluster = createChecksum(self, vm_cluster, data_volume_1_cluster, 'datadiskdevice_1')
    storagePools_to_avoid = [data_volume_1_cluster.storage, destinationPool_for_root_disk.name]
    destinationPool_for_data_disk1 = GetDestinationPool(self, storagePools_to_avoid, 'CLUSTER')
    MigrateDataVolume(self, data_volume_1_cluster, destinationPool_for_data_disk1, islive=True)
    self.assertEqual(vm_cluster.state, 'Running', 'Check VM State is running or not.')
    vm_cluster.detach_volume(self.userapiclient, data_volume_clust_1)
    compareChecksum(self, checksum_random_data_cluster, 'datadiskdevice_1', virt_machine=None, disk=data_volume_1_cluster, new_vm=True)
    vm_cluster.delete(self.apiclient)
    self.assertEqual(VirtualMachine.list(self.apiclient, id=vm_cluster.id), None, 'VM list should be empty')
    data_volume_clust_1.delete(self.apiclient)
    self.assertEqual(Volume.list(self.apiclient, id=data_volume_clust_1.id), None, 'Volume list should be empty')
    self.assertEqual(Volume.list(self.apiclient, id=root_volume_cluster.id), None, 'Volume list should be empty')
    return
